{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fb6cf-dba0-43fc-9fb8-d116681ecb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers \n",
    "!pip install torch\n",
    "!pip install speechbrain\n",
    "!pip install torchaudio==2.7.0\n",
    "!pip install datasets==3.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5789741f-9f5d-45a1-9606-8250fae65f7b",
   "metadata": {},
   "source": [
    "### Speaker encoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a50fd-48c5-4076-afc7-5e9a82f3be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset, Audio\n",
    "import torchaudio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b065c2-790d-4c4b-b663-fc12651dd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1006c38-7a29-47b0-a334-86150b246fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpeakerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=80, hidden_dim=256, output_dim=512, num_layers=4, num_heads=8, dropout=0.1):\n",
    "        super(CustomSpeakerEncoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.conv3 = nn.Conv1d(256, hidden_dim, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,  \n",
    "                nhead=num_heads,    \n",
    "                dropout=dropout      \n",
    "            ),\n",
    "            num_layers=num_layers  \n",
    "        )\n",
    "        \n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, mel_spectrogram):\n",
    "        x = F.relu(self.bn1(self.conv1(mel_spectrogram))) \n",
    "        x = F.relu(self.bn2(self.conv2(x)))                \n",
    "        x = F.relu(self.bn3(self.conv3(x)))               \n",
    "        \n",
    "        x = x.transpose(1, 2)  # Shape: [batch_size, time_steps, hidden_dim]\n",
    "        \n",
    "        transformer_out = self.transformer_encoder(x) \n",
    "        \n",
    "        attention_weights = F.softmax(self.attention(transformer_out), dim=1)  \n",
    "        pooled = torch.sum(attention_weights * transformer_out, dim=1)  \n",
    "        \n",
    "        x = F.relu(self.fc1(pooled))  \n",
    "        x = self.dropout(x)\n",
    "        speaker_embedding = self.fc2(x)         \n",
    "        speaker_embedding = F.normalize(speaker_embedding, p=2, dim=1)\n",
    "        \n",
    "        return speaker_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae65de-73d9-4eaf-93f3-2290dcb0752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, num_speakers=100):\n",
    "        super(SpeakerClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, num_speakers)\n",
    "        \n",
    "    def forward(self, embeddings):        \n",
    "        return self.fc(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60102754-9c70-4e20-b6a8-4de9b444fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(waveform, sample_rate=16000, n_mels=80):\n",
    "    if not isinstance(waveform, torch.Tensor):\n",
    "        waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "    \n",
    "    waveform = waveform.unsqueeze(0)\n",
    "    \n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=400,\n",
    "        hop_length=160,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    mel_spec = mel_transform(waveform)\n",
    "    mel_spec = torch.log1p(mel_spec)\n",
    "    mel_spec = mel_spec.squeeze(0)\n",
    "    \n",
    "    return mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd92ca-d660-4c4d-a528-687dcb6642d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        mel_spec = extract_mel_spectrogram(item[\"audio\"][\"array\"])\n",
    "        speaker_id = item[\"speaker_id\"]\n",
    "        return mel_spec, speaker_id\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad mel spectrograms to same length in batch\n",
    "    mel_specs, speaker_ids = zip(*batch)    \n",
    "    max_len = max([mel.shape[1] for mel in mel_specs])\n",
    "    max_len = min(max_len, 500)  # Limit max length\n",
    "    \n",
    "    # Pad or truncate\n",
    "    padded_mels = []\n",
    "    for mel in mel_specs:\n",
    "        if mel.shape[1] > max_len:\n",
    "            mel = mel[:, :max_len]  \n",
    "        else:\n",
    "            pad_len = max_len - mel.shape[1]\n",
    "            if pad_len > 0:\n",
    "                mel = F.pad(mel, (0, pad_len), value=0) \n",
    "        padded_mels.append(mel)\n",
    "    \n",
    "    mel_batch = torch.stack(padded_mels)\n",
    "    speaker_ids = torch.tensor(speaker_ids, dtype=torch.long)\n",
    "    \n",
    "    return mel_batch, speaker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416265a-6696-4daa-8aac-7eb0b7595d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/voxpopuli\", \"nl\", split=\"train\", streaming=True)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "subset = list(dataset.take(1000))\n",
    "\n",
    "unique_speakers = list(set([item[\"speaker_id\"] for item in subset]))\n",
    "speaker_to_id = {speaker: idx for idx, speaker in enumerate(unique_speakers)}\n",
    "num_speakers = len(unique_speakers)\n",
    "print(f\"Number of unique speakers: {num_speakers}\")\n",
    "\n",
    "for item in subset:\n",
    "    item[\"speaker_id\"] = speaker_to_id[item[\"speaker_id\"]]\n",
    "\n",
    "speaker_dataset = SpeakerDataset(subset)\n",
    "train_loader = DataLoader(\n",
    "    speaker_dataset, \n",
    "    batch_size=4,  \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "speaker_encoder = CustomSpeakerEncoder(\n",
    "    input_dim=80,\n",
    "    hidden_dim=128,  \n",
    "    output_dim=512,\n",
    "    num_layers=2, \n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "classifier = SpeakerClassifier(\n",
    "    embedding_dim=512,\n",
    "    num_speakers=num_speakers\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(speaker_encoder.parameters()) + list(classifier.parameters()),\n",
    "    lr=1e-5\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "speaker_encoder.train()\n",
    "classifier.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for mel_specs, speaker_ids in pbar:\n",
    "        mel_specs = mel_specs.to(device)\n",
    "        speaker_ids = speaker_ids.to(device)\n",
    "        \n",
    "        embeddings = speaker_encoder(mel_specs)\n",
    "        logits = classifier(embeddings)\n",
    "        \n",
    "        loss = criterion(logits, speaker_ids)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        correct += predicted.eq(speaker_ids).sum().item()\n",
    "        total += speaker_ids.size(0)\n",
    "        \n",
    "        if total % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"acc\": f\"{100.*correct/total:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}%\")\n",
    "\n",
    "output_file = \"pretrained_speaker_encoder_6.pt\"\n",
    "torch.save(speaker_encoder.state_dict(), outputFile)\n",
    "print(\"Saved to: \", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418716d9-9970-4ffc-8058-d1c46f0254fb",
   "metadata": {},
   "source": [
    "### Integration with text2speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fd3af-5127-4fa8-bc52-dbc953c6dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file_name = \"pretrained_speaker_encoder_6.pt\"\n",
    "huggingface_token = \"TOKEN\"\n",
    "model_name = \"ACCOUNTNAME/MODELNAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4852ab1-54cc-4d46-bd4d-caf9e94622a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_speaker_encoder = CustomSpeakerEncoder(\n",
    "    input_dim=80,\n",
    "    hidden_dim=128,\n",
    "    output_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "custom_speaker_encoder.load_state_dict(torch.load(encoder_file_name, map_location=device))\n",
    "custom_speaker_encoder.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7dfd38-f5de-4e2f-874f-921a290a11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio, Dataset\n",
    "\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from huggingface_hub import login\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "dataset = load_dataset(\"facebook/voxpopuli\", \"nl\", split=\"train\", streaming=True)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset = dataset.take(4000)\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        mel_spec = extract_mel_spectrogram(waveform)\n",
    "        mel_spec = mel_spec.unsqueeze(0).to(device)         \n",
    "        speaker_embedding = custom_speaker_encoder(mel_spec)\n",
    "        speaker_embedding = speaker_embedding.squeeze().cpu().numpy()\n",
    "    \n",
    "    return speaker_embedding\n",
    "\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    \n",
    "    example = processor(\n",
    "        text=example[\"normalized_text\"],\n",
    "        audio_target=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # strip off the batch dimension    \n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "\n",
    "    # use SpeechBrain to obtain x-vector    \n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
    "    \n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n",
    "\n",
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < 200\n",
    "\n",
    "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
    "dataset = Dataset.from_list(list(dataset))\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "    \n",
    "    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "        \n",
    "        # collate the inputs and targets into a batch\n",
    "        batch = processor.pad(input_ids=input_ids, labels=label_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100)\n",
    "        \n",
    "        # not used during fine-tuning\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "        \n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor([len(feature[\"input_values\"]) for feature in label_features])\n",
    "            target_lengths = target_lengths.new(\n",
    "                [length - length % model.config.reduction_factor for length in target_lengths]\n",
    "            )\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "        \n",
    "        # also add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "data_collator = TTSDataCollatorWithPadding(processor=processor)\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\n",
    "model.config.use_cache = False\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"speecht5_finetuned_voxpopuli_nl_custom6\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    #warmup_steps=500,    \n",
    "    warmup_steps=50,\n",
    "    #max_steps=4000,\n",
    "    max_steps=2000,    \n",
    "    #gradient_checkpointing=True,    \n",
    "    gradient_checkpointing=False,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=2,\n",
    "    #save_steps=1000,\n",
    "    #eval_steps=1000,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,    \n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "login(token=huggingface_token)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(model_name)\n",
    "processor.save_pretrained(model_name)\n",
    "\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5abac-4a1f-4997-a66b-1375d1576f8d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac748984-4bbd-4e38-bfb3-d466258020d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file_name = \"pretrained_speaker_encoder_6.pt\"\n",
    "huggingface_token = \"TOKEN\"\n",
    "model_name = \"ACCOUNTNAME/MODELNAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700eb1f8-1af9-4811-8ebe-6f125bca2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_speaker_encoder = CustomSpeakerEncoder(\n",
    "    input_dim=80,\n",
    "    hidden_dim=128,\n",
    "    output_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "custom_speaker_encoder.load_state_dict(torch.load(encoder_file_name, map_location=device))\n",
    "custom_speaker_encoder.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3a34a-c9b7-46ac-bef9-33d427a9d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset, Audio as AudioFeature\n",
    "import numpy as np\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(model_name)\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# vocoder for converting to audio\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)\n",
    "vocoder.eval()\n",
    "\n",
    "def synthesize_speech(text, reference_audio_array, sample_rate=16000):    \n",
    "    mel_spec = extract_mel_spectrogram(reference_audio_array, sample_rate)\n",
    "    mel_spec = mel_spec.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = custom_speaker_encoder(mel_spec)\n",
    "        \n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        spectrogram = model.generate_speech(input_ids, speaker_embeddings, vocoder=vocoder)\n",
    "    \n",
    "    audio = spectrogram.cpu().numpy()\n",
    "    \n",
    "    return audio\n",
    "\n",
    "dataset = load_dataset(\"facebook/voxpopuli\", \"nl\", split=\"test\", streaming=True)\n",
    "dataset = dataset.cast_column(\"audio\", AudioFeature(sampling_rate=16000))\n",
    "\n",
    "it = iter(dataset)\n",
    "for _ in range(2):\n",
    "    next(it)\n",
    "reference_example = next(it)\n",
    "reference_audio = reference_example['audio']['array']\n",
    "\n",
    "text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n",
    "\n",
    "audio = synthesize_speech(text, reference_audio)\n",
    "\n",
    "from IPython.display import Audio as IPythonAudio\n",
    "IPythonAudio(audio, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ruudenv)",
   "language": "python",
   "name": "ruudenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
